import os
import math
import numpy as np
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras import layers
from tensorflow.python.keras.callbacks import ModelCheckpoint
from tensorflow.python.keras.utils.data_utils import Sequence
from tensorflow.python.keras.utils.np_utils import to_categorical
from sklearn.utils import shuffle

# This code generates the final model that will be used in the application, using the vectors
# generated by 2.vectorGenerator.py

# We load the vectors from the folders
vecBenigni = ["vecBenigni/" + fileName for fileName in os.listdir("vecBenigni")]
vecMaligni = ["vecMaligni/" + fileName for fileName in os.listdir("vecMaligni")]

nBenigni = len(vecBenigni)
nMaligni = len(vecMaligni)

x = np.array(vecBenigni + vecMaligni)  # Array containing all the vectors paths
# We create the class labels for the vectors, 0=benign, 1=ransomware
y = np.ones(nBenigni + nMaligni)  # Initially we create an array of 1s
y[0:nBenigni] = 0  # Then we set to 0 the positions corresponding to the benign vectors
y[nBenigni:nBenigni + nMaligni] = 1  # Then we set to 1 the rest of the array (just to be sure)

# We apply one-hot encoding to the labels, so all the labels will be numerical values
y = to_categorical(y, num_classes=2)

# We split the dataset into training, validation and testing
ind = np.arange(nBenigni + nMaligni)  # We create an array of indexes
np.random.shuffle(ind)  # We order them randomly
p = np.array([0.8, 0.1, 0.1])  # We define the percentage that we will be using to split the dataset
#                                80% training, 10% validation, 10% testing

# We split the dataset in 3 parts
sub = np.split(ind, (len(ind) * p[:-1].cumsum()).astype(int))
trainInds = sub[0]
valInds = sub[1]
np.save("testInds", sub[2])

# Then we apply this division to the arrays x and y created before
x_train = x[trainInds]
x_val = x[valInds]

y_train = y[trainInds]
y_val = y[valInds]


# We use the Sequence class from keras to make the training and validation process more safe
# this class allows to do multiprocessing of large quantities of data in a safer way

# The Sequence class guarantees that the network will only train once on each item per epoch
# this is not guaranteed if we don't use this class
# This is the default implementation of the Sequence class, that can be found in the Keras documentation
class TrainSequence(Sequence):
    def __init__(self, x, y, batch_size):
        self.x, self.y = shuffle(x, y)
        self.batch_size = batch_size

    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]

        return np.array([
            np.load(file_name)
            for file_name in batch_x]), np.array(batch_y)

    # This definition is required for the program to compile correctly
    def on_epoch_end(self):
        pass


# We have to define a Sequence class for the validation process too
class TrainSequenceVal(TrainSequence):
    def __init__(self, x, y, batch_size):
        super().__init__(x, y, batch_size)
        self.x, self.y = x, y
        self.batch_size = batch_size


# We define the size of the batches of data that will be used for the training and validation process
batch_size = 1000
# We instantiate the sequences for training and validation
trainingSeqGen = TrainSequence(x_train, y_train, batch_size)
validationSeqGen = TrainSequenceVal(x_val, y_val, batch_size)

# We define the model
model = Sequential()

# We create a neural network consisting of 4 layers, the first layer has 128 neurons, the next layers have
# half the number of neurons of the previous layer

# We use the Rectified Linear Unit function to check if a neuron's output is passed to the next layer
model.add(layers.InputLayer(input_shape=(50,)))  # We have 50 input data for every item
model.add(layers.Dense(128, activation='relu'))
# We also use a dropout layer, with a 20% dropout rate
# Dropout helps to reduce overfitting, making the model more accurate
# To find the best dropout rate, various rates were tested
model.add(layers.Dropout(0.2))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(16, activation='relu'))
# In the final layer, we apply the softmax function to the results to generate a probability for the 2 classes
model.add(layers.Dense(2, activation='softmax'))

# We apply the rmsprop optimization algorithm to the model
model.compile(optimizer="rmsprop", loss='categorical_crossentropy', metrics=['accuracy'])

# We specify where the weights will be saved
weightPath = "weights/weights-{epoch:02d}-{val_accuracy:.2f}.hdf5"

# At the end of each epoch, the model will be tested on the validation data, and if the accuracy is higher
# than the previous saved model's accuracy, the model's weights will be saved
# We specify this by creating a checkpoint that will be inserted in the callbacks for the model
checkpoint = ModelCheckpoint(filepath=weightPath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')

# Then we start the training process
model.fit(x=trainingSeqGen,
          epochs=500,  # The model will be trained for 500 epochs
          steps_per_epoch=len(trainingSeqGen),
          verbose=1,
          validation_data=validationSeqGen,
          validation_steps=len(validationSeqGen),
          workers=8,
          use_multiprocessing=True,
          callbacks=[checkpoint])

# NOTE: Best accuracy on the validation set: 99.798%
